{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.7 (default, May 21 2020, 14:57:43) \n",
      "[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)]\n",
      "1.6.0\n",
      "10.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "print(sys.version)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy_estimator():\n",
    "    def __init__(self, env):\n",
    "        self.n_inputs = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        \n",
    "        # Define network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.n_inputs, 16), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(16, self.n_outputs),\n",
    "            nn.Softmax(dim=-1))\n",
    "    \n",
    "    def predict(self, state):\n",
    "        action_probs = self.network(torch.FloatTensor(state))\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4861, 0.5139], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4861, 0.5139], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "s = env.reset()\n",
    "pe = policy_estimator(env)\n",
    "print(pe.predict(s))\n",
    "print(pe.network(torch.FloatTensor(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    r = np.array([gamma**i * rewards[i] \n",
    "                  for i in range(len(rewards))])\n",
    "    # Reverse the array direction for cumsum and then\n",
    "    # revert back to the original order\n",
    "    r = r[::-1].cumsum()[::-1]\n",
    "    return r - r.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, policy_estimator, num_episodes=2000,\n",
    "              batch_size=10, gamma=0.99):\n",
    "\n",
    "    # Set up lists to hold results\n",
    "    total_rewards = []\n",
    "    batch_rewards = []\n",
    "    batch_actions = []\n",
    "    batch_states = []\n",
    "    batch_counter = 1\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(policy_estimator.network.parameters(), \n",
    "                           lr=0.01)\n",
    "    \n",
    "    action_space = np.arange(env.action_space.n)\n",
    "    for ep in range(num_episodes):\n",
    "        s_0 = env.reset()\n",
    "        states = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        complete = False\n",
    "        while complete == False:\n",
    "            # Get actions and convert to numpy array\n",
    "            action_probs = policy_estimator.predict(s_0).detach().numpy()\n",
    "            action = np.random.choice(action_space, p=action_probs)\n",
    "            s_1, r, complete, _ = env.step(action)\n",
    "            \n",
    "            states.append(s_0)\n",
    "            rewards.append(r)\n",
    "            actions.append(action)\n",
    "            s_0 = s_1\n",
    "            \n",
    "            # If complete, batch data\n",
    "            if complete:\n",
    "                batch_rewards.extend(discount_rewards(rewards, gamma))\n",
    "                batch_states.extend(states)\n",
    "                batch_actions.extend(actions)\n",
    "                batch_counter += 1\n",
    "                total_rewards.append(sum(rewards))\n",
    "                \n",
    "                # If batch is complete, update network\n",
    "                if batch_counter == batch_size:\n",
    "                    optimizer.zero_grad()\n",
    "                    state_tensor = torch.FloatTensor(batch_states)\n",
    "                    reward_tensor = torch.FloatTensor(batch_rewards)\n",
    "                    # Actions are used as indices, must be LongTensor\n",
    "                    action_tensor = torch.LongTensor(batch_actions)\n",
    "                    \n",
    "                    print(\"state_tensor: \", state_tensor)\n",
    "                    print(\"reward_tensor: \", reward_tensor)\n",
    "                    print(\"action_tensor: \", action_tensor)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    logprob = torch.log(\n",
    "                        policy_estimator.predict(state_tensor))\n",
    "                    selected_logprobs = reward_tensor * \\\n",
    "                        logprob[np.arange(len(action_tensor)), action_tensor]\n",
    "                    loss = -selected_logprobs.mean()\n",
    "                    \n",
    "                    # Calculate gradients\n",
    "                    loss.backward()\n",
    "                    # Apply gradients\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    batch_rewards = []\n",
    "                    batch_actions = []\n",
    "                    batch_states = []\n",
    "                    batch_counter = 1\n",
    "                    \n",
    "                # Print running average\n",
    "                #print(\"\\rEp: {} Average of last 10: {:.2f}\".format(\n",
    "                #    ep + 1, np.mean(total_rewards[-10:])), end=\"\")\n",
    "                \n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_tensor:  tensor([[-0.0428, -0.0263,  0.0209, -0.0289],\n",
      "        [-0.0434,  0.1685,  0.0203, -0.3149],\n",
      "        [-0.0400, -0.0269,  0.0140, -0.0159],\n",
      "        ...,\n",
      "        [ 1.1978, -0.5550, -0.0770,  0.2334],\n",
      "        [ 1.1867, -0.7490, -0.0723,  0.5009],\n",
      "        [ 1.1717, -0.5529, -0.0623,  0.1863]])\n",
      "reward_tensor:  tensor([ 56.6990,  55.6990,  54.7090,  ..., -29.4929, -29.6310, -29.7677])\n",
      "action_tensor:  tensor([1, 0, 1,  ..., 0, 1, 0])\n",
      "state_tensor:  tensor([[ 0.0359,  0.0432,  0.0045, -0.0279],\n",
      "        [ 0.0367, -0.1520,  0.0039,  0.2662],\n",
      "        [ 0.0337, -0.3472,  0.0092,  0.5601],\n",
      "        ...,\n",
      "        [ 1.6718,  0.5271,  0.0468,  0.0112],\n",
      "        [ 1.6824,  0.7215,  0.0470, -0.2664],\n",
      "        [ 1.6968,  0.5258,  0.0416,  0.0408]])\n",
      "reward_tensor:  tensor([ 53.7110,  52.7110,  51.7210,  ..., -29.4929, -29.6310, -29.7677])\n",
      "action_tensor:  tensor([0, 0, 1,  ..., 1, 0, 0])\n",
      "state_tensor:  tensor([[-0.0327, -0.0145, -0.0256, -0.0068],\n",
      "        [-0.0330, -0.2092, -0.0258,  0.2777],\n",
      "        [-0.0372, -0.0138, -0.0202, -0.0230],\n",
      "        ...,\n",
      "        [-0.3565,  0.5859,  0.0122, -0.3159],\n",
      "        [-0.3448,  0.7809,  0.0058, -0.6048],\n",
      "        [-0.3291,  0.5857, -0.0063, -0.3102]])\n",
      "reward_tensor:  tensor([ 56.6990,  55.6990,  54.7090,  ..., -29.4929, -29.6310, -29.7677])\n",
      "action_tensor:  tensor([0, 1, 0,  ..., 1, 0, 1])\n",
      "state_tensor:  tensor([[-3.2110e-02, -3.0081e-02, -3.5228e-03, -1.4590e-02],\n",
      "        [-3.2712e-02, -2.2515e-01, -3.8146e-03,  2.7698e-01],\n",
      "        [-3.7215e-02, -2.9976e-02,  1.7250e-03, -1.6904e-02],\n",
      "        ...,\n",
      "        [ 2.3202e+00,  1.4598e+00, -5.3273e-02,  1.6764e-01],\n",
      "        [ 2.3494e+00,  1.6556e+00, -4.9920e-02, -1.4136e-01],\n",
      "        [ 2.3825e+00,  1.4613e+00, -5.2748e-02,  1.3517e-01]])\n",
      "reward_tensor:  tensor([ 56.6990,  55.6990,  54.7090,  ..., -28.8287, -29.0735, -29.3160])\n",
      "action_tensor:  tensor([0, 1, 0,  ..., 1, 0, 1])\n",
      "state_tensor:  tensor([[-0.0259, -0.0380, -0.0327, -0.0269],\n",
      "        [-0.0267, -0.2326, -0.0333,  0.2553],\n",
      "        [-0.0313, -0.4273, -0.0282,  0.5373],\n",
      "        ...,\n",
      "        [ 2.3214,  1.8079,  0.0419, -0.4926],\n",
      "        [ 2.3576,  1.6122,  0.0321, -0.1870],\n",
      "        [ 2.3898,  1.8068,  0.0283, -0.4694]])\n",
      "reward_tensor:  tensor([ 56.6990,  55.6990,  54.7090,  ..., -29.5703, -29.7245, -29.8772])\n",
      "action_tensor:  tensor([0, 0, 1,  ..., 0, 1, 0])\n",
      "state_tensor:  tensor([[ 0.0304, -0.0032,  0.0399,  0.0076],\n",
      "        [ 0.0303, -0.1988,  0.0401,  0.3126],\n",
      "        [ 0.0263, -0.0043,  0.0463,  0.0328],\n",
      "        ...,\n",
      "        [ 1.3882,  0.1812,  0.0763,  0.4218],\n",
      "        [ 1.3918,  0.3752,  0.0848,  0.1542],\n",
      "        [ 1.3993,  0.5690,  0.0879, -0.1106]])\n",
      "reward_tensor:  tensor([ 51.8310,  50.8310,  49.8410,  ..., -29.4929, -29.6310, -29.7677])\n",
      "action_tensor:  tensor([0, 1, 1,  ..., 1, 1, 0])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6cd9ad3206cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m smoothed_rewards = [np.mean(rewards[i-window:i+1]) if i > window \n\u001b[1;32m      4\u001b[0m                     else np.mean(rewards[:i+1]) for i in range(len(rewards))]\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-dc442f76c9a9>\u001b[0m in \u001b[0;36mreinforce\u001b[0;34m(env, policy_estimator, num_episodes, batch_size, gamma)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Get actions and convert to numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0ms_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rewards = reinforce(env, pe)\n",
    "window = 10\n",
    "smoothed_rewards = [np.mean(rewards[i-window:i+1]) if i > window \n",
    "                    else np.mean(rewards[:i+1]) for i in range(len(rewards))]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(rewards)\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.ylabel('Total Rewards')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
